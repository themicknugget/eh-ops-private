apiVersion: inference.eh-ops.io/v1alpha1
kind: InferenceBackend
metadata:
  name: llamacpp-cpu
  namespace: inference
spec:
  image:
    repository: ghcr.io/ggml-org/llama.cpp
    tag: server
  port: 8080
  command:
    - /app/llama-server
  args:
    - -m
    - $(HF_SOURCE)
    - --host
    - 0.0.0.0
    - --port
    - "8080"
    - --metrics
    - -c
    - "8192"
  readinessPath: /health
