{{- if .Values.name -}}
{{- $port := include "inference-model.port" . | int -}}
{{- $modelDir := .Values.storage.modelDir | default .Values.name -}}
{{- $sharedPvc := .Values.storage.sharedPvc | default "model-cache" -}}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Values.name }}
  labels:
    {{- include "inference-model.labels" . | nindent 4 }}
spec:
  replicas: 0  # KubeElasti controls scaling
  strategy:
    type: Recreate  # Only one pod per model (limited GPU)
  selector:
    matchLabels:
      {{- include "inference-model.labels" . | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include "inference-model.labels" . | nindent 8 }}
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: {{ $port | quote }}
        prometheus.io/path: /metrics
    spec:
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- if .Values.download.enabled }}
      initContainers:
        - name: ensure-model
          image: {{ .Values.download.image | default "python:3.11-slim" }}
          command:
            - sh
            - -c
            - |
              set -e

              MODEL_DIR=/models/{{ $modelDir }}

              # 1. Already downloaded? Fast exit
              if [ -f "${MODEL_DIR}/.ready" ]; then
                echo "Model already cached"
                exit 0
              fi

              echo "Waiting for download job to complete..."

              # 2. Wait for Job to complete (with timeout)
              TIMEOUT=86400  # 24 hours
              elapsed=0
              while [ ! -f "${MODEL_DIR}/.ready" ] && [ ! -f "${MODEL_DIR}/.failed" ]; do
                sleep 10
                elapsed=$((elapsed + 10))

                # Show progress every 5 minutes
                if [ $((elapsed % 300)) -eq 0 ]; then
                  echo "Still waiting... (${elapsed}s elapsed)"
                fi

                # Timeout - fallback to downloading ourselves
                if [ $elapsed -ge $TIMEOUT ]; then
                  echo "Timeout waiting for download job, downloading ourselves..."
                  break
                fi
              done

              # 3. Check if Job succeeded
              if [ -f "${MODEL_DIR}/.ready" ]; then
                echo "Model ready!"
                exit 0
              fi

              # 4. Job failed or timeout - download ourselves
              echo "Download job not available, downloading model to ${MODEL_DIR}..."

              # Install huggingface-cli with CLI extras
              pip install --user "huggingface_hub[cli]"
              export PATH="/tmp/.local/bin:$PATH"

              mkdir -p "${MODEL_DIR}"
              rm -f "${MODEL_DIR}/.failed"
              touch "${MODEL_DIR}/.downloading"

              # Background: update lock timestamp every minute
              while true; do
                touch "${MODEL_DIR}/.downloading"
                sleep 60
              done &
              heartbeat_pid=$!

              # Background: print download progress every 60s with transfer rate
              last_size=0
              while [ ! -f "${MODEL_DIR}/.ready" ]; do
                size=$(du -s "${MODEL_DIR}" 2>/dev/null | cut -f1)
                if [ -n "$size" ] && [ "$last_size" -gt 0 ]; then
                  diff=$((size - last_size))
                  # diff is in 512-byte blocks, multiply by 512 to get bytes
                  bytes=$((diff * 512))
                  # Convert to MB/s (over 60 second interval)
                  rate=$((bytes / 60 / 1024 / 1024))
                  size_gb=$((size * 512 / 1024 / 1024 / 1024))
                  echo "${size_gb}G downloaded (~${rate} MB/s)"
                elif [ -n "$size" ]; then
                  size_gb=$((size * 512 / 1024 / 1024 / 1024))
                  echo "${size_gb}G downloaded"
                fi
                last_size=${size:-0}
                sleep 60
              done &
              progress_pid=$!

              hf download "{{ .Values.source.huggingface }}" \
                --local-dir "${MODEL_DIR}"

              kill $heartbeat_pid $progress_pid 2>/dev/null || true
              rm -f "${MODEL_DIR}/.downloading"
              touch "${MODEL_DIR}/.ready"
              echo "Model downloaded successfully"
          env:
            - name: HF_HOME
              value: /models
            - name: HOME
              value: /tmp
            {{- if .Values.download.tokenSecret }}
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: {{ .Values.download.tokenSecret }}
                  key: {{ .Values.download.tokenSecretKey | default "token" }}
            {{- end }}
          volumeMounts:
            - name: model-cache
              mountPath: /models
            - name: tmp
              mountPath: /tmp
      {{- end }}
      containers:
        - name: inference
          image: {{ include "inference-model.image" . }}
          imagePullPolicy: IfNotPresent
          args:
            {{- $hfSource := printf "/models/%s" $modelDir -}}
            {{- include "inference-model.backendArgs" . | replace "$(HF_SOURCE)" $hfSource | nindent 12 }}
          ports:
            - name: http
              containerPort: {{ $port }}
              protocol: TCP
          {{- with .Values.resources }}
          resources:
            {{- toYaml . | nindent 12 }}
          {{- end }}
          env:
            - name: HF_HOME
              value: /models
            - name: HF_SOURCE
              value: {{ .Values.source.huggingface }}
            - name: MODEL_NAME
              value: {{ .Values.name }}
            {{- if .Values.source.contextSize }}
            - name: LLAMA_ARG_CTX_SIZE
              value: {{ .Values.source.contextSize | quote }}
            {{- end }}
            {{- /* Backend env vars */ -}}
            {{- include "inference-model.backendEnv" . | nindent 12 }}
            {{- /* User env vars (can override backend defaults) */ -}}
            {{- with .Values.env }}
            {{- toYaml . | nindent 12 }}
            {{- end }}
          securityContext:
            {{- include "inference-model.securityContext" . | nindent 12 }}
          volumeMounts:
            - name: model-cache
              mountPath: /models
            {{- if .Values.promptTemplate }}
            - name: prompt-template
              mountPath: /prompt/templates
              readOnly: true
            {{- end }}
            {{- include "inference-model.volumeMounts" . | nindent 12 }}
          livenessProbe:
            httpGet:
              path: {{ include "inference-model.readinessPath" . }}
              port: {{ $port }}
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: {{ include "inference-model.readinessPath" . }}
              port: {{ $port }}
            initialDelaySeconds: 10
            periodSeconds: 5
          startupProbe:
            httpGet:
              path: {{ include "inference-model.readinessPath" . }}
              port: {{ $port }}
            failureThreshold: 180  # 30 minutes max for large models
            periodSeconds: 10
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: {{ $sharedPvc }}
        - name: tmp
          emptyDir: {}
        {{- if .Values.promptTemplate }}
        - name: prompt-template
          configMap:
            name: {{ .Values.name }}-template
        {{- end }}
        {{- include "inference-model.volumes" . | nindent 8 }}
{{- end -}}
