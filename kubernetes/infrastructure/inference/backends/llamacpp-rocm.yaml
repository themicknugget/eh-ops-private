apiVersion: inference.eh-ops.io/v1alpha1
kind: InferenceBackend
metadata:
  name: llamacpp-rocm
  namespace: inference
spec:
  image:
    repository: ghcr.io/ggml-org/llama.cpp
    tag: server-rocm
  port: 8080
  args:
    - llama-server
    - -m
    - $(HF_SOURCE)
    - --host
    - 0.0.0.0
    - --port
    - "8080"
    - --metrics
  readinessPath: /health
  env:
    - name: HSA_OVERRIDE_GFX_VERSION
      value: "11.5.1"
    - name: ROCBLAS_USE_HIPBLASLT
      value: "1"
  securityContext:
    capabilities:
      add:
        - SYS_PTRACE
    seccompProfile:
      type: Unconfined
  volumes:
    - name: dri
      hostPath:
        path: /dev/dri
    - name: kfd
      hostPath:
        path: /dev/kfd
  volumeMounts:
    - name: dri
      mountPath: /dev/dri
    - name: kfd
      mountPath: /dev/kfd
