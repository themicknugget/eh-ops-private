---
apiVersion: batch/v1
kind: Job
metadata:
  name: download-test-models
  namespace: llama-cpp
spec:
  ttlSecondsAfterFinished: 3600
  template:
    spec:
      nodeSelector:
        workload-type: gpu
      tolerations:
        - key: workload-type
          operator: Equal
          value: heavy
          effect: NoSchedule
      restartPolicy: Never
      containers:
        - name: downloader
          image: docker.io/kyuz0/amd-strix-halo-toolboxes:vulkan-radv
          securityContext:
            runAsUser: 0
          command:
            - /bin/sh
            - -c
            - |
              cd /models

              echo "=== Downloading 3 test models ==="
              echo "1. Nemotron-3-Nano-30B-A3B Q8_0 (31GB) - Excellent quality, 1M context"
              echo "2. Kimi-Linear-48B-A3B Q8_0 (49GB) - Excellent quality, 1M context"
              echo "3. GLM-4.7 UD-IQ2_M (114GB, 3 parts) - 2-bit quant, 200K context"
              echo ""

              # Download Nemotron Q8_0 (single file, 31GB)
              echo "[1/3] Downloading Nemotron-3-Nano-30B-A3B Q8_0..."
              curl -L -C - -o Nemotron-3-Nano-30B-A3B-Q8_0.gguf \
                "https://huggingface.co/unsloth/Nemotron-3-Nano-30B-A3B-GGUF/resolve/main/Nemotron-3-Nano-30B-A3B-Q8_0.gguf" &

              # Download Kimi-Linear Q8_0 (single file, 49GB)
              echo "[2/3] Downloading Kimi-Linear-48B-A3B Q8_0..."
              curl -L -C - -o Kimi-Linear-48B-A3B-Instruct-Q8_0.gguf \
                "https://huggingface.co/AaryanK/Kimi-Linear-48B-A3B-Instruct-GGUF/resolve/main/Kimi-Linear-48B-A3B-Instruct.q8_0.gguf" &

              # Download GLM-4.7 UD-IQ2_M (3 parts, 114GB total)
              echo "[3/3] Downloading GLM-4.7 UD-IQ2_M (3 parts)..."
              curl -L -C - -o GLM-4.7-UD-IQ2_M-00001-of-00003.gguf \
                "https://huggingface.co/unsloth/GLM-4.7-GGUF/resolve/main/UD-IQ2_M/GLM-4.7-UD-IQ2_M-00001-of-00003.gguf" &
              curl -L -C - -o GLM-4.7-UD-IQ2_M-00002-of-00003.gguf \
                "https://huggingface.co/unsloth/GLM-4.7-GGUF/resolve/main/UD-IQ2_M/GLM-4.7-UD-IQ2_M-00002-of-00003.gguf" &
              curl -L -C - -o GLM-4.7-UD-IQ2_M-00003-of-00003.gguf \
                "https://huggingface.co/unsloth/GLM-4.7-GGUF/resolve/main/UD-IQ2_M/GLM-4.7-UD-IQ2_M-00003-of-00003.gguf" &

              wait
              echo ""
              echo "=== Download complete! ==="
              echo ""
              echo "Nemotron:"
              ls -lh /models/Nemotron*.gguf 2>/dev/null || echo "  (not found)"
              echo ""
              echo "Kimi-Linear:"
              ls -lh /models/Kimi-Linear*.gguf 2>/dev/null || echo "  (not found)"
              echo ""
              echo "GLM-4.7:"
              ls -lh /models/GLM-4.7*.gguf 2>/dev/null || echo "  (not found)"
          volumeMounts:
            - name: models
              mountPath: /models
      volumes:
        - name: models
          persistentVolumeClaim:
            claimName: llama-models
