apiVersion: inference.eh-ops.io/v1alpha1
kind: InferenceBackend
metadata:
  name: llamacpp-vulkan-moe
  namespace: inference
spec:
  image:
    repository: ghcr.io/ggml-org/llama.cpp
    tag: server-vulkan
  port: 8080
  args:
    - llama-server
    - -m
    - $(HF_SOURCE)
    - --host
    - 0.0.0.0
    - --port
    - "8080"
    - --metrics
  readinessPath: /health
  env:
    - name: LLAMA_HIP_UMA
      value: "ON"
    - name: LLAMA_ARG_N_GPU_LAYERS
      value: "99"
    - name: LLAMA_ARG_FLASH_ATTN
      value: "1"
    - name: LLAMA_ARG_CACHE_TYPE_K
      value: "q4_0"
    - name: LLAMA_ARG_CACHE_TYPE_V
      value: "q4_0"
    - name: LLAMA_ARG_THREADS
      value: "12"
    - name: LLAMA_ARG_BATCH_SIZE
      value: "4096"
    - name: LLAMA_ARG_UBATCH_SIZE
      value: "1024"
    - name: LLAMA_ARG_ENDPOINT_METRICS
      value: "1"
  securityContext:
    capabilities:
      add:
        - SYS_PTRACE
    seccompProfile:
      type: Unconfined
  volumes:
    - name: dri
      hostPath:
        path: /dev/dri
    - name: kfd
      hostPath:
        path: /dev/kfd
  volumeMounts:
    - name: dri
      mountPath: /dev/dri
    - name: kfd
      mountPath: /dev/kfd
