---
apiVersion: batch/v1
kind: Job
metadata:
  name: download-minimax-m2
  namespace: llama-cpp
spec:
  ttlSecondsAfterFinished: 3600
  template:
    spec:
      nodeSelector:
        workload-type: gpu
      tolerations:
        - key: workload-type
          operator: Equal
          value: heavy
          effect: NoSchedule
      restartPolicy: Never
      containers:
        - name: downloader
          image: docker.io/kyuz0/amd-strix-halo-toolboxes:vulkan-radv
          securityContext:
            runAsUser: 0
          command:
            - /bin/sh
            - -c
            - |
              cd /models
              echo "Downloading MiniMax-M2.1-Q3_K_XL (109GB, 3 parts)..."

              # Download in parallel
              curl -L -C - -o MiniMaxAI_MiniMax-M2.1-Q3_K_XL-00001-of-00003.gguf \
                "https://huggingface.co/bartowski/MiniMaxAI_MiniMax-M2.1-GGUF/resolve/main/MiniMaxAI_MiniMax-M2.1-Q3_K_XL/MiniMaxAI_MiniMax-M2.1-Q3_K_XL-00001-of-00003.gguf" &

              curl -L -C - -o MiniMaxAI_MiniMax-M2.1-Q3_K_XL-00002-of-00003.gguf \
                "https://huggingface.co/bartowski/MiniMaxAI_MiniMax-M2.1-GGUF/resolve/main/MiniMaxAI_MiniMax-M2.1-Q3_K_XL/MiniMaxAI_MiniMax-M2.1-Q3_K_XL-00002-of-00003.gguf" &

              curl -L -C - -o MiniMaxAI_MiniMax-M2.1-Q3_K_XL-00003-of-00003.gguf \
                "https://huggingface.co/bartowski/MiniMaxAI_MiniMax-M2.1-GGUF/resolve/main/MiniMaxAI_MiniMax-M2.1-Q3_K_XL/MiniMaxAI_MiniMax-M2.1-Q3_K_XL-00003-of-00003.gguf" &

              wait
              echo "Download complete!"
              ls -lh /models/MiniMaxAI*.gguf
          volumeMounts:
            - name: models
              mountPath: /models
      volumes:
        - name: models
          persistentVolumeClaim:
            claimName: llama-models
