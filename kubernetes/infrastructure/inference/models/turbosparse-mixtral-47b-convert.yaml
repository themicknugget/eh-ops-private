# One-time Job: download TurboSparse-Mixtral safetensors, convert to GGUF, quantize to Q4_K_M.
# TurboSparse only ships safetensors â€” no GGUF available upstream.
# The HelmRelease (turbosparse-mixtral-47b.yaml) waits for .ready before starting inference.
apiVersion: batch/v1
kind: Job
metadata:
  name: turbosparse-mixtral-47b-convert
  namespace: inference
spec:
  ttlSecondsAfterFinished: 604800  # 7 days
  template:
    spec:
      restartPolicy: OnFailure
      nodeSelector:
        kubernetes.io/hostname: shadow
      tolerations:
        - key: amd.com/gpu
          operator: Exists
      containers:
        - name: convert
          image: ghcr.io/ggml-org/llama.cpp:full
          command:
            - sh
            - -c
            - |
              set -e
              MODEL_DIR=/models/turbosparse-mixtral-47b
              GGUF_RAW="${MODEL_DIR}/turbosparse-mixtral-47b.f16.gguf"
              GGUF_OUT="${MODEL_DIR}/turbosparse-mixtral-47b.Q4_K_M.gguf"

              if [ -f "${MODEL_DIR}/.ready" ]; then
                echo "Already converted"
                exit 0
              fi

              mkdir -p "${MODEL_DIR}"

              pip install --quiet "huggingface_hub[cli]" hf-transfer
              export PATH="/root/.local/bin:$PATH"
              export HF_HUB_ENABLE_HF_TRANSFER=1

              echo "Downloading safetensors..."
              huggingface-cli download PowerInfer/TurboSparse-Mixtral \
                --local-dir "${MODEL_DIR}/src" \
                --token "${HF_TOKEN}"

              echo "Converting to GGUF..."
              python3 /app/convert_hf_to_gguf.py \
                "${MODEL_DIR}/src" \
                --outfile "${GGUF_RAW}"

              echo "Quantizing to Q4_K_M..."
              /app/llama-quantize "${GGUF_RAW}" "${GGUF_OUT}" Q4_K_M

              rm -f "${GGUF_RAW}"
              rm -rf "${MODEL_DIR}/src"
              touch "${MODEL_DIR}/.ready"
              echo "Done: ${GGUF_OUT}"
          env:
            - name: HF_HOME
              value: /models
            - name: HOME
              value: /root
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: token
          resources:
            requests:
              memory: 96Gi
              cpu: "4"
            limits:
              memory: 120Gi
          volumeMounts:
            - name: model-cache
              mountPath: /models
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: model-cache
