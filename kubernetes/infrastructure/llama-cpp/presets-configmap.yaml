---
apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-presets
  namespace: llama-cpp
data:
  presets.ini: |
    # Global defaults for all models
    [*]
    cont-batching = on
    flash-attn = on
    n-gpu-layers = 999
    metrics = on
    cache-type-k = q8_0
    cache-type-v = q8_0

    # MiniMax M2.1 - 101GB Q3_K_XL (3 parts), 1M native context MoE
    [unsloth/MiniMax-M2.1-GGUF:UD-Q3_K_XL]
    hf-repo = unsloth/MiniMax-M2.1-GGUF:UD-Q3_K_XL
    ctx-size = 131072
    parallel = 1

    # GLM-4.7 - 114GB UD-IQ2_M, 334B MoE, 202K context
    [unsloth/GLM-4.7-GGUF:UD-IQ2_M]
    hf-repo = unsloth/GLM-4.7-GGUF:UD-IQ2_M
    ctx-size = 131072
    parallel = 1

    # Llama 3.2 3B - small fast model for testing
    [unsloth/Llama-3.2-3B-Instruct-GGUF:Q4_K_M]
    hf-repo = unsloth/Llama-3.2-3B-Instruct-GGUF:Q4_K_M
    ctx-size = 32768
    parallel = 4

    # Llama 3.2 1B - tiny model for quick tests
    [unsloth/Llama-3.2-1B-Instruct-GGUF:Q4_K_M]
    hf-repo = unsloth/Llama-3.2-1B-Instruct-GGUF:Q4_K_M
    ctx-size = 32768
    parallel = 4

    # Nemotron 30B MoE - 31GB Q8_0, 1M native context
    [unsloth/Nemotron-3-Nano-30B-A3B-GGUF:Q8_0]
    hf-repo = unsloth/Nemotron-3-Nano-30B-A3B-GGUF:Q8_0
    ctx-size = 65536
    parallel = 4

    # Kimi Linear 48B MoE - 49GB Q8_0, 1M native context
    [AaryanK/Kimi-Linear-48B-A3B-Instruct-GGUF:Q8_0]
    hf-repo = AaryanK/Kimi-Linear-48B-A3B-Instruct-GGUF:Q8_0
    ctx-size = 65536
    parallel = 4
