{{- if .Values.name -}}
{{- $port := include "inference-model.port" . | int -}}
{{- $modelDir := .Values.storage.modelDir | default .Values.name -}}
{{- $sharedPvc := .Values.storage.sharedPvc | default "model-cache" -}}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Values.name }}
  labels:
    {{- include "inference-model.labels" . | nindent 4 }}
spec:
  replicas: 0  # KubeElasti controls scaling
  strategy:
    type: Recreate  # Only one pod per model (limited GPU)
  selector:
    matchLabels:
      {{- include "inference-model.labels" . | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include "inference-model.labels" . | nindent 8 }}
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: {{ $port | quote }}
        prometheus.io/path: /metrics
    spec:
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- if .Values.download.enabled }}
      initContainers:
        - name: ensure-model
          image: {{ .Values.download.image | default "python:3.11-slim" }}
          command:
            - sh
            - -c
            - |
              set -e

              MODEL_DIR=/models/{{ $modelDir }}

              # 1. Already downloaded? Fast exit
              if [ -f "${MODEL_DIR}/.ready" ]; then
                echo "Model already cached"
                exit 0
              fi

              echo "Waiting for download job to complete..."

              # 2. Wait for Job to complete (with timeout)
              TIMEOUT=86400  # 24 hours
              elapsed=0
              while [ ! -f "${MODEL_DIR}/.ready" ] && [ ! -f "${MODEL_DIR}/.failed" ]; do
                sleep 10
                elapsed=$((elapsed + 10))

                # Show progress every 5 minutes
                if [ $((elapsed % 300)) -eq 0 ]; then
                  echo "Still waiting... (${elapsed}s elapsed)"
                fi

                # Timeout - fallback to downloading ourselves
                if [ $elapsed -ge $TIMEOUT ]; then
                  echo "Timeout waiting for download job, downloading ourselves..."
                  break
                fi
              done

              # 3. Check if Job succeeded
              if [ -f "${MODEL_DIR}/.ready" ]; then
                echo "Model ready!"
                exit 0
              fi

              # 4. Job failed or timeout - download ourselves
              echo "Download job not available, downloading model to ${MODEL_DIR}..."

              # Install huggingface-cli with CLI extras
              pip install --user "huggingface_hub[cli]" hf-transfer
              export PATH="/tmp/.local/bin:$PATH"
              export HF_HUB_ENABLE_HF_TRANSFER=1

              mkdir -p "${MODEL_DIR}"
              rm -f "${MODEL_DIR}/.failed"
              touch "${MODEL_DIR}/.downloading"

              # Background: update lock timestamp every minute
              while true; do
                touch "${MODEL_DIR}/.downloading"
                sleep 60
              done &
              heartbeat_pid=$!

              # Background: print download progress every 60s with transfer rate
              last_bytes=0
              while [ ! -f "${MODEL_DIR}/.ready" ]; do
                bytes=$(du -sb "${MODEL_DIR}" 2>/dev/null | cut -f1)
                if [ -n "$bytes" ] && [ "$last_bytes" -gt 0 ]; then
                  diff=$((bytes - last_bytes))
                  size_gb=$((bytes / 1024 / 1024 / 1024))
                  # Calculate rate - show MB/s if >= 1, otherwise KB/s
                  rate_mb=$((diff / 60 / 1024 / 1024))
                  if [ "$rate_mb" -ge 1 ]; then
                    echo "${size_gb}G downloaded (~${rate_mb} MB/s)"
                  else
                    rate_kb=$((diff / 60 / 1024))
                    echo "${size_gb}G downloaded (~${rate_kb} KB/s)"
                  fi
                elif [ -n "$bytes" ]; then
                  size_gb=$((bytes / 1024 / 1024 / 1024))
                  echo "${size_gb}G downloaded"
                fi
                last_bytes=${bytes:-0}
                sleep 60
              done &
              progress_pid=$!

              hf download "{{ .Values.source.huggingface }}" \
              {{- if .Values.download.include }}
                --include "{{ .Values.download.include }}" \
              {{- end }}
                --local-dir "${MODEL_DIR}"

              kill $heartbeat_pid $progress_pid 2>/dev/null || true
              rm -f "${MODEL_DIR}/.downloading"
              touch "${MODEL_DIR}/.ready"
              echo "Model downloaded successfully"
          env:
            - name: HF_HOME
              value: /models
            - name: HOME
              value: /tmp
            {{- if .Values.download.tokenSecret }}
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: {{ .Values.download.tokenSecret }}
                  key: {{ .Values.download.tokenSecretKey | default "token" }}
            {{- end }}
          volumeMounts:
            - name: model-cache
              mountPath: /models
            - name: tmp
              mountPath: /tmp
      {{- end }}
      containers:
        - name: inference
          image: {{ include "inference-model.image" . }}
          imagePullPolicy: Always
          {{- with include "inference-model.backendCommand" . }}
          command:
            {{- . | nindent 12 }}
          {{- end }}
          args:
            {{- $hfSource := printf "/models/%s" $modelDir -}}
            {{- $backendName := include "inference-model.backendName" . -}}
            {{- /* Use specific shard file when provided (required for split GGUF models) */ -}}
            {{- if .Values.storage.modelFile -}}
            {{- $hfSource = printf "/models/%s/%s" $modelDir .Values.storage.modelFile -}}
            {{- end -}}
            {{- include "inference-model.backendArgs" . | replace "$(HF_SOURCE)" $hfSource | nindent 12 }}
          ports:
            - name: http
              containerPort: {{ $port }}
              protocol: TCP
          {{- $backendName2 := include "inference-model.backendName" . -}}
          {{- $gpuBackends := list "llamacpp-rocm" "llamacpp-vulkan" "llamacpp-vulkan-moe" -}}
          {{- $userRes := .Values.resources | default dict -}}
          {{- $res := $userRes -}}
          {{- if has $backendName2 $gpuBackends -}}
          {{- $gpuLimit := dict "amd.com/gpu" "1" -}}
          {{- $backendRes := dict "limits" $gpuLimit "requests" $gpuLimit -}}
          {{- $res = merge $userRes $backendRes -}}
          {{- end -}}
          {{- if $res }}
          resources:
            {{- toYaml $res | nindent 12 }}
          {{- end }}
          env:
            - name: HF_HOME
              value: /models
            - name: HF_SOURCE
              value: {{ .Values.source.huggingface }}
            - name: MODEL_NAME
              value: {{ .Values.name }}
            {{- if .Values.source.contextSize }}
            - name: LLAMA_ARG_CTX_SIZE
              value: {{ .Values.source.contextSize | quote }}
            {{- end }}
            {{- /* Backend env vars */ -}}
            {{- include "inference-model.backendEnv" . | nindent 12 }}
            {{- /* User env vars (can override backend defaults) */ -}}
            {{- with .Values.env }}
            {{- toYaml . | nindent 12 }}
            {{- end }}
          securityContext:
            {{- include "inference-model.securityContext" . | nindent 12 }}
          volumeMounts:
            - name: model-cache
              mountPath: /models
            {{- if .Values.promptTemplate }}
            - name: prompt-template
              mountPath: /prompt/templates
              readOnly: true
            {{- end }}
            {{- include "inference-model.volumeMounts" . | nindent 12 }}
          livenessProbe:
            httpGet:
              path: {{ include "inference-model.readinessPath" . }}
              port: {{ $port }}
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: {{ include "inference-model.readinessPath" . }}
              port: {{ $port }}
            initialDelaySeconds: 10
            periodSeconds: 5
          startupProbe:
            httpGet:
              path: {{ include "inference-model.readinessPath" . }}
              port: {{ $port }}
            failureThreshold: 180  # 30 minutes max for large models
            periodSeconds: 10
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: {{ $sharedPvc }}
        - name: tmp
          emptyDir: {}
        {{- if .Values.promptTemplate }}
        - name: prompt-template
          configMap:
            name: {{ .Values.name }}-template
        {{- end }}
        {{- include "inference-model.volumes" . | nindent 8 }}
{{- end -}}
