---
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: tailscale-subnet-router-hostnetwork
spec:
  rules:
    - name: add-hostnetwork
      match:
        any:
          - resources:
              kinds:
                - StatefulSet
              names:
                - "ts-subnet-router-*"
              namespaces:
                - tailscale
      mutate:
        patchStrategicMerge:
          spec:
            template:
              spec:
                hostNetwork: true
                dnsPolicy: Default
                hostAliases:
                  - ip: "10.96.0.1"
                    hostnames:
                      - "kubernetes.default.svc"
                      - "kubernetes.default.svc.cluster.local"
                      - "kubernetes"
                affinity:
                  podAntiAffinity:
                    requiredDuringSchedulingIgnoredDuringExecution:
                      - labelSelector:
                          matchExpressions:
                            - key: app
                              operator: In
                              values:
                                - ts-subnet-router
                        topologyKey: kubernetes.io/hostname
                nodeSelector:
                  node-role.kubernetes.io/control-plane: ""
                tolerations:
                  - key: node-role.kubernetes.io/control-plane
                    operator: Exists
                    effect: NoSchedule
                containers:
                  - name: tailscale
                    resources:
                      requests:
                        cpu: 10m
                        memory: 64Mi
                      limits:
                        cpu: 500m
                        memory: 128Mi
                    lifecycle:
                      postStart:
                        exec:
                          # MASQUERADE non-Tailscale traffic going through tailscale0 so
                          # the source IP becomes this Connector's Tailscale IP. Required
                          # because WireGuard AllowedIPs only assign subnet CIDRs to ONE
                          # of the 3 Connector replicas — the other two would have their
                          # forwarded packets (pod CIDR, LAN IPs) dropped by the receiving
                          # peer's WireGuard validation.
                          command:
                            - sh
                            - -c
                            - >-
                              iptables-nft -t nat -C POSTROUTING -o tailscale0 ! -s 100.64.0.0/10 -j MASQUERADE 2>/dev/null ||
                              iptables-nft -t nat -A POSTROUTING -o tailscale0 ! -s 100.64.0.0/10 -j MASQUERADE;
                              sysctl -w net.ipv4.conf.tailscale0.route_localnet=1;
                              iptables-nft -t nat -C PREROUTING -i tailscale0 -s 100.64.0.0/10 -d 192.168.1.64 -p udp --dport 53 -j DNAT --to-destination 127.0.0.1:1053 2>/dev/null ||
                              iptables-nft -t nat -A PREROUTING -i tailscale0 -s 100.64.0.0/10 -d 192.168.1.64 -p udp --dport 53 -j DNAT --to-destination 127.0.0.1:1053;
                              iptables-nft -t nat -C PREROUTING -i tailscale0 -s 100.64.0.0/10 -d 192.168.1.64 -p tcp --dport 53 -j DNAT --to-destination 127.0.0.1:1053 2>/dev/null ||
                              iptables-nft -t nat -A PREROUTING -i tailscale0 -s 100.64.0.0/10 -d 192.168.1.64 -p tcp --dport 53 -j DNAT --to-destination 127.0.0.1:1053;
                              iptables-nft -t nat -C PREROUTING -i tailscale0 -s 100.64.0.0/10 -d 192.168.1.65 -p tcp --dport 80 -j DNAT --to-destination 127.0.0.1:30080 2>/dev/null ||
                              iptables-nft -t nat -A PREROUTING -i tailscale0 -s 100.64.0.0/10 -d 192.168.1.65 -p tcp --dport 80 -j DNAT --to-destination 127.0.0.1:30080;
                              iptables-nft -t nat -C PREROUTING -i tailscale0 -s 100.64.0.0/10 -d 192.168.1.65 -p tcp --dport 443 -j DNAT --to-destination 127.0.0.1:30443 2>/dev/null ||
                              iptables-nft -t nat -A PREROUTING -i tailscale0 -s 100.64.0.0/10 -d 192.168.1.65 -p tcp --dport 443 -j DNAT --to-destination 127.0.0.1:30443;
                              true
                  # socat re-originates connections to LB VIPs so Cilium socket-level LB
                  # intercepts them — forwarded Tailscale traffic bypasses Cilium's eBPF
                  - name: socat-proxy
                    image: alpine/socat:1.8.0.1
                    command: ["sh", "-c"]
                    args:
                      - |
                        socat UDP4-LISTEN:1053,fork,reuseaddr,bind=127.0.0.1 UDP4:192.168.1.64:53 &
                        socat TCP4-LISTEN:1053,fork,reuseaddr,bind=127.0.0.1 TCP4:192.168.1.64:53 &
                        socat TCP4-LISTEN:30080,fork,reuseaddr,bind=127.0.0.1 TCP4:192.168.1.65:80 &
                        socat TCP4-LISTEN:30443,fork,reuseaddr,bind=127.0.0.1 TCP4:192.168.1.65:443 &
                        wait
                    resources:
                      requests:
                        cpu: 10m
                        memory: 16Mi
                      limits:
                        cpu: 100m
                        memory: 32Mi
