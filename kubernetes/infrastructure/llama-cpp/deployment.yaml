---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-server
  namespace: llama-cpp
  labels:
    app: llama-server
spec:
  replicas: 1
  strategy:
    type: Recreate  # Required for RWO PVC
  selector:
    matchLabels:
      app: llama-server
  template:
    metadata:
      labels:
        app: llama-server
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      # Schedule on shadow node (AMD Strix Halo GPU)
      nodeSelector:
        workload-type: gpu
      tolerations:
        - key: workload-type
          operator: Equal
          value: heavy
          effect: NoSchedule

      containers:
        - name: llama-server
          image: docker.io/kyuz0/amd-strix-halo-toolboxes:vulkan-radv

          command:
            - llama-server
          args:
            - "--host"
            - "0.0.0.0"
            - "--port"
            - "8080"
            # Single model mode - load specific model at startup
            - "--model"
            - "/models/Llama-3.2-3B-Instruct-Q4_K_M.gguf"
            # Critical performance flags for ROCm/Strix Halo
            - "--no-mmap"          # Required for ROCm - prevents catastrophic slowdown
            - "-ngl"
            - "999"                # Offload all layers to GPU
            - "-fa"
            - "1"                  # Enable Flash Attention
            - "--cont-batching"    # Enable continuous batching
            - "--metrics"          # Enable Prometheus metrics endpoint
            - "-c"
            - "8192"               # Default context size
            - "--parallel"
            - "4"                  # Parallel request slots
            - "-fit"
            - "off"                # Disable auto-fit - hangs on Strix Halo gfx1151
            - "--verbose"          # Enable verbose logging for diagnostics

          env:
            # Vulkan environment for RADV driver
            - name: VK_ICD_FILENAMES
              value: "/usr/share/vulkan/icd.d/radeon_icd.x86_64.json"
            - name: RADV_PERFTEST
              value: "gpl"

          ports:
            - name: http
              containerPort: 8080
              protocol: TCP

          volumeMounts:
            - name: models
              mountPath: /models
            - name: dev-kfd
              mountPath: /dev/kfd
            - name: dev-dri
              mountPath: /dev/dri

          resources:
            requests:
              memory: "32Gi"
              cpu: "8"
            limits:
              memory: "120Gi"
              cpu: "24"

          securityContext:
            privileged: true  # Required for full GPU access

          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 120
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 10

          readinessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5

      volumes:
        - name: models
          persistentVolumeClaim:
            claimName: llama-models
        - name: dev-kfd
          hostPath:
            path: /dev/kfd
        - name: dev-dri
          hostPath:
            path: /dev/dri
