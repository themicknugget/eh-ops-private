# Template for adding a new LLM model
# Copy this file and customize the values below

apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: MODEL_NAME  # e.g., llama-3-8b, mistral-7b, phi-4
  namespace: inference
spec:
  interval: 5m
  chart:
    spec:
      chart: ./kubernetes/infrastructure/inference/chart
      sourceRef:
        kind: GitRepository
        name: flux-system
        namespace: flux-system
      interval: 5m
  values:
    # Required: Model name (used for deployment, service names)
    name: MODEL_NAME

    # Backend: llamacpp-vulkan, llamacpp-rocm, vllm
    backend: llamacpp-vulkan

    # Model source from HuggingFace
    source:
      huggingface: ORG/MODEL-ID  # e.g., meta-llama/Llama-3.1-8B-Instruct

    # Resource requirements
    resources:
      requests:
        memory: 16Gi
        cpu: "2"
      limits:
        memory: 24Gi

    # Model caching storage
    storage:
      enabled: true
      size: 50Gi

    # Scale-to-zero configuration
    scaling:
      cooldownPeriod: 600  # Seconds idle before scale to 0
      maxReplicas: 1       # Max pods (1 for single-GPU nodes)
      # Optional: custom Prometheus query for scaling
      # triggerQuery: |
      #   sum(rate(envoy_cluster_upstream_rq_total{cluster_name=~".*MODEL_NAME.*"}[1m])) or vector(0)
      # triggerThreshold: "0.1"

    # Node placement (required for GPU nodes)
    nodeSelector:
      kubernetes.io/hostname: shadow

    # GPU tolerations
    tolerations:
      - key: amd.com/gpu
        operator: Exists

    # Optional: Custom environment variables
    # env:
    #   - name: LLAMA_HIP_UMA
    #     value: "ON"

    # Optional: Gateway API route
    # httpRoute:
    #   enabled: true
    #   hostname: MODEL_NAME.example.com
    #   gateway:
    #     name: cilium
    #     namespace: cilium

---
# Quick reference for common models:

# 7-8B models (16-24GB memory):
# - meta-llama/Llama-3.1-8B-Instruct
# - mistralai/Mistral-7B-Instruct-v0.3
# - Qwen/Qwen2.5-7B-Instruct

# 14B models (20-28GB memory):
# - microsoft/Phi-4-mini-instruct

# 70B models (40-50GB memory with Q4):
# - meta-llama/Llama-3.1-70B-Instruct
# - Qwen/Qwen2.5-72B-Instruct
