# One-time Job: download TurboSparse-Mistral safetensors, convert to GGUF, quantize to Q4_K_M.
# TurboSparse only ships safetensors â€” no GGUF available upstream.
# The HelmRelease (turbosparse-mistral-7b.yaml) waits for .ready before starting inference.
apiVersion: batch/v1
kind: Job
metadata:
  name: turbosparse-mistral-7b-convert
  namespace: inference
spec:
  suspend: true
  ttlSecondsAfterFinished: 604800  # 7 days
  template:
    spec:
      restartPolicy: OnFailure
      nodeSelector:
        kubernetes.io/hostname: shadow
      tolerations:
        - key: amd.com/gpu
          operator: Exists
      containers:
        - name: convert
          image: ghcr.io/ggml-org/llama.cpp:full
          command:
            - sh
            - -c
            - |
              set -e
              MODEL_DIR=/models/turbosparse-mistral-7b
              GGUF_RAW="$${MODEL_DIR}/turbosparse-mistral-7b.f16.gguf"
              GGUF_OUT="$${MODEL_DIR}/turbosparse-mistral-7b.Q4_K_M.gguf"

              if [ -f "$${MODEL_DIR}/.ready" ]; then
                echo "Already converted"
                exit 0
              fi

              mkdir -p "$${MODEL_DIR}"

              pip install --quiet "huggingface_hub[cli]" hf-transfer
              export PATH="/root/.local/bin:$PATH"
              export HF_HUB_ENABLE_HF_TRANSFER=1

              echo "Downloading safetensors..."
              huggingface-cli download PowerInfer/TurboSparse-Mistral-Instruct \
                --local-dir "$${MODEL_DIR}/src" \
                --token "$${HF_TOKEN}"

              echo "Converting to GGUF..."
              python3 /app/convert_hf_to_gguf.py \
                "$${MODEL_DIR}/src" \
                --outfile "$${GGUF_RAW}"

              echo "Quantizing to Q4_K_M..."
              /app/llama-quantize "$${GGUF_RAW}" "$${GGUF_OUT}" Q4_K_M

              rm -f "$${GGUF_RAW}"
              rm -rf "$${MODEL_DIR}/src"
              touch "$${MODEL_DIR}/.ready"
              echo "Done: $${GGUF_OUT}"
          env:
            - name: HF_HOME
              value: /models
            - name: HOME
              value: /root
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: token
          resources:
            requests:
              memory: 32Gi
              cpu: "4"
            limits:
              memory: 48Gi
          volumeMounts:
            - name: model-cache
              mountPath: /models
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: model-cache
