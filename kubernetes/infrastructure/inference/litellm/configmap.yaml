apiVersion: v1
kind: ConfigMap
metadata:
  name: litellm-config
  namespace: inference
data:
  config.yaml: |
    general_settings:
      # CRITICAL: Disable all background health checks to prevent scale-up of idle backends
      # KubeElasti handles scale-to-zero, health checks would trigger unwanted scale-up
      background_health_checks: false

    litellm_settings:
      timeout: 1800
      stream_timeout: 1800
      disable_adding_openai_public_endpoints_to_db: true

    router_settings:
      # Reduced retries - KubeElasti scales up in ~2-3 min, no need for 30 min of retries
      num_retries: 20
      retry_after: 15
      allowed_fails: 9999
      routing_strategy: simple-shuffle

    model_list:
      - model_name: minimax-m25-reap
        litellm_params:
          model: openai/minimax-m25-reap
          api_base: http://minimax-m25-reap.inference.svc.cluster.local:8080/v1
          api_key: dummy
          timeout: 1800
        model_info:
          mode: chat
          disable_background_health_check: true

      - model_name: minimax-m25-reap-139b
        litellm_params:
          model: openai/minimax-m25-reap-139b
          api_base: http://minimax-m25-reap-139b.inference.svc.cluster.local:8080/v1
          api_key: dummy
          timeout: 1800
        model_info:
          mode: chat
          disable_background_health_check: true

      - model_name: deepseek-r1-distill-70b
        litellm_params:
          model: openai/deepseek-r1-distill-70b
          api_base: http://deepseek-r1-distill-70b.inference.svc.cluster.local:8080/v1
          api_key: dummy
          timeout: 1800
        model_info:
          mode: chat
          disable_background_health_check: true

      - model_name: llama-4-scout
        litellm_params:
          model: openai/llama-4-scout
          api_base: http://llama-4-scout.inference.svc.cluster.local:8080/v1
          api_key: dummy
          timeout: 1800
        model_info:
          mode: chat
          disable_background_health_check: true

      - model_name: llama-3-3-70b
        litellm_params:
          model: openai/llama-3-3-70b
          api_base: http://llama-3-3-70b.inference.svc.cluster.local:8080/v1
          api_key: dummy
          timeout: 1800
        model_info:
          mode: chat
          disable_background_health_check: true

      - model_name: qwen2-5-72b
        litellm_params:
          model: openai/qwen2-5-72b
          api_base: http://qwen2-5-72b.inference.svc.cluster.local:8080/v1
          api_key: dummy
          timeout: 1800
        model_info:
          mode: chat
          disable_background_health_check: true

      - model_name: devstral-2-123b
        litellm_params:
          model: openai/devstral-2-123b
          api_base: http://devstral-2-123b.inference.svc.cluster.local:8080/v1
          api_key: dummy
          timeout: 1800
        model_info:
          mode: chat
          disable_background_health_check: true

      - model_name: qwen3-235b-a22b
        litellm_params:
          model: openai/qwen3-235b-a22b
          api_base: http://qwen3-235b-a22b.inference.svc.cluster.local:8080/v1
          api_key: dummy
          timeout: 1800
        model_info:
          mode: chat
          disable_background_health_check: true

      - model_name: qwen3-next-80b
        litellm_params:
          model: openai/qwen3-next-80b
          api_base: http://qwen3-next-80b.inference.svc.cluster.local:8080/v1
          api_key: dummy
          timeout: 1800
        model_info:
          mode: chat
          disable_background_health_check: true

      - model_name: qwen3-coder-next
        litellm_params:
          model: openai/qwen3-coder-next
          api_base: http://qwen3-coder-next.inference.svc.cluster.local:8080/v1
          api_key: dummy
          timeout: 1800
        model_info:
          mode: chat
          disable_background_health_check: true

      - model_name: gemma-3-27b
        litellm_params:
          model: openai/gemma-3-27b
          api_base: http://gemma-3-27b.inference.svc.cluster.local:8080/v1
          api_key: dummy
          timeout: 1800
        model_info:
          mode: chat
          disable_background_health_check: true

      - model_name: mistral-small-3-1-24b
        litellm_params:
          model: openai/mistral-small-3-1-24b
          api_base: http://mistral-small-3-1-24b.inference.svc.cluster.local:8080/v1
          api_key: dummy
          timeout: 1800
        model_info:
          mode: chat
          disable_background_health_check: true

      - model_name: phi-4-reasoning-plus
        litellm_params:
          model: openai/phi-4-reasoning-plus
          api_base: http://phi-4-reasoning-plus.inference.svc.cluster.local:8080/v1
          api_key: dummy
          timeout: 1800
        model_info:
          mode: chat
          disable_background_health_check: true

      - model_name: phi-4-mini
        litellm_params:
          model: openai/phi-4-mini
          api_base: http://phi-4-mini.inference.svc.cluster.local:8080/v1
          api_key: dummy
          timeout: 1800
        model_info:
          mode: chat
          disable_background_health_check: true

      - model_name: gpt-oss-120b
        litellm_params:
          model: openai/gpt-oss-120b
          api_base: http://gpt-oss-120b.inference.svc.cluster.local:8080/v1
          api_key: dummy
          timeout: 1800
        model_info:
          mode: chat
          disable_background_health_check: true

      - model_name: gpt-oss-20b
        litellm_params:
          model: openai/gpt-oss-20b
          api_base: http://gpt-oss-20b.inference.svc.cluster.local:8080/v1
          api_key: dummy
          timeout: 1800
        model_info:
          mode: chat
          disable_background_health_check: true

      - model_name: qwen2-5-vl-72b
        litellm_params:
          model: openai/qwen2-5-vl-72b
          api_base: http://qwen2-5-vl-72b.inference.svc.cluster.local:8080/v1
          api_key: dummy
          timeout: 1800
        model_info:
          mode: chat
          disable_background_health_check: true

      - model_name: qwen3-vl-30b
        litellm_params:
          model: openai/qwen3-vl-30b
          api_base: http://qwen3-vl-30b.inference.svc.cluster.local:8080/v1
          api_key: dummy
          timeout: 1800
        model_info:
          mode: chat
          disable_background_health_check: true

      - model_name: qwen2-5-coder-32b
        litellm_params:
          model: openai/qwen2-5-coder-32b
          api_base: http://qwen2-5-coder-32b.inference.svc.cluster.local:8080/v1
          api_key: dummy
          timeout: 1800
        model_info:
          mode: chat
          disable_background_health_check: true

      - model_name: deepseek-r1-0528-8b
        litellm_params:
          model: openai/deepseek-r1-0528-8b
          api_base: http://deepseek-r1-0528-8b.inference.svc.cluster.local:8080/v1
          api_key: dummy
          timeout: 1800
        model_info:
          mode: chat
          disable_background_health_check: true

      - model_name: command-a
        litellm_params:
          model: openai/command-a
          api_base: http://command-a.inference.svc.cluster.local:8080/v1
          api_key: dummy
          timeout: 1800
        model_info:
          mode: chat
          disable_background_health_check: true

      - model_name: glm-4-7-flash
        litellm_params:
          model: openai/glm-4-7-flash
          api_base: http://glm-4-7-flash.inference.svc.cluster.local:8080/v1
          api_key: dummy
          timeout: 1800
        model_info:
          mode: chat
          disable_background_health_check: true

      - model_name: whisper-large-v3-turbo
        litellm_params:
          model: openai/whisper-large-v3-turbo
          api_base: http://whisper-large-v3-turbo.inference.svc.cluster.local:8080/v1
          api_key: dummy
        model_info:
          mode: audio_transcription
          disable_background_health_check: true

      - model_name: orpheus-3b
        litellm_params:
          model: openai/orpheus-3b
          api_base: http://orpheus-3b.inference.svc.cluster.local:5005/v1
          api_key: dummy
        model_info:
          mode: audio_speech
          disable_background_health_check: true

      - model_name: kittentts
        litellm_params:
          model: openai/kittentts
          api_base: http://kittentts.inference.svc.cluster.local:8080/v1
          api_key: dummy
        model_info:
          mode: audio_speech
          disable_background_health_check: true

