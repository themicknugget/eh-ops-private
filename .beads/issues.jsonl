{"id":"eh-ops-09p","title":"Layer 0: Talos Linux Base Foundation","status":"closed","priority":0,"issue_type":"epic","created_at":"2025-12-24T17:05:04.666651311Z","updated_at":"2025-12-24T17:57:59.887202156Z","closed_at":"2025-12-24T17:57:59.887202156Z","close_reason":"Layer 0 complete: Talos v1.12.0 cluster bootstrapped with 8 nodes (3 control plane + 5 workers). All nodes healthy, CSRs approved, control plane operational."}
{"id":"eh-ops-0aq","title":"Deploy Cilium CNI with Gateway API","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T08:44:29.385512294-07:00","updated_at":"2025-12-25T01:30:14.91432507Z","closed_at":"2025-12-25T01:30:14.91432507Z","close_reason":"Cilium v1.18.0 deployed with Gateway API support. All 8 nodes Ready. L2 announcements configured but VIP not externally accessible due to kernel 6.18.1-talos compatibility issue - L2 announcement database populated (192.168.1.50 on eno1) but BPF L2 responder shows responses_sent=0. Root cause: kernel 6.18 has breaking neighbor table changes affecting ARP responses. Recommendations: Use MetalLB/kube-vip alternative or wait for Cilium kernel 6.18 compatibility fix. See Cilium Discussion #36829 for kernel 6.12 fix (acceleration: best-effort).","dependencies":[{"issue_id":"eh-ops-0aq","depends_on_id":"eh-ops-9zq","type":"blocks","created_at":"2025-12-23T08:49:19.686509236-07:00","created_by":"micknugget","metadata":"{}"}]}
{"id":"eh-ops-0xd","title":"Deploy Grafana with dashboards and auth","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-23T08:45:40.490168569-07:00","updated_at":"2025-12-27T17:09:32.539129956Z","closed_at":"2025-12-27T17:09:32.539129956Z","close_reason":"Closed","dependencies":[{"issue_id":"eh-ops-0xd","depends_on_id":"eh-ops-d9s","type":"blocks","created_at":"2025-12-23T08:48:41.368187348-07:00","created_by":"micknugget","metadata":"{}"}]}
{"id":"eh-ops-195","title":"Configure SOPS secrets management","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T08:45:11.228043168-07:00","updated_at":"2025-12-27T00:31:22.964366842Z","closed_at":"2025-12-27T00:31:22.964366842Z","close_reason":"Closed","dependencies":[{"issue_id":"eh-ops-195","depends_on_id":"eh-ops-2b2","type":"blocks","created_at":"2025-12-23T08:48:24.876922694-07:00","created_by":"micknugget","metadata":"{}"}]}
{"id":"eh-ops-1be","title":"Deploy Gateway Inference Extension","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-23T08:46:29.230280368-07:00","updated_at":"2025-12-28T22:29:39.798261721Z","closed_at":"2025-12-28T22:29:39.798261721Z","close_reason":"Not needed - requires non-Cilium gateway (using Cilium Gateway). Also, KServe architecture was replaced with direct llama.cpp server which doesn't need inference routing extension.","dependencies":[{"issue_id":"eh-ops-1be","depends_on_id":"eh-ops-63k","type":"blocks","created_at":"2025-12-23T08:49:08.063277026-07:00","created_by":"micknugget","metadata":"{}"}]}
{"id":"eh-ops-2b2","title":"Configure storage: Local-Path and NFS provisioners","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T08:45:04.872695827-07:00","updated_at":"2025-12-27T00:08:13.645867789Z","closed_at":"2025-12-27T00:08:13.645867789Z","close_reason":"Closed","dependencies":[{"issue_id":"eh-ops-2b2","depends_on_id":"eh-ops-j12","type":"blocks","created_at":"2025-12-23T08:48:19.795336321-07:00","created_by":"micknugget","metadata":"{}"}]}
{"id":"eh-ops-2ek","title":"Deploy Tailscale VPN for subnet routing","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T08:45:17.641512851-07:00","updated_at":"2025-12-27T01:10:10.091073097Z","closed_at":"2025-12-27T01:10:10.091073097Z","close_reason":"Closed","dependencies":[{"issue_id":"eh-ops-2ek","depends_on_id":"eh-ops-195","type":"blocks","created_at":"2025-12-23T08:48:29.966723828-07:00","created_by":"micknugget","metadata":"{}"}]}
{"id":"eh-ops-2k9","title":"Deploy SNMP Exporter for network devices","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-23T08:45:53.405816332-07:00","updated_at":"2025-12-27T21:43:10.00775986Z","closed_at":"2025-12-27T21:43:10.00775986Z","close_reason":"Closed","dependencies":[{"issue_id":"eh-ops-2k9","depends_on_id":"eh-ops-did","type":"blocks","created_at":"2025-12-23T08:48:51.538624078-07:00","created_by":"micknugget","metadata":"{}"}]}
{"id":"eh-ops-38u","title":"Fix llama-cpp ServiceMonitor label for Prometheus discovery","description":"## Problem\nThe llama-cpp ServiceMonitor was missing the 'release: kube-prometheus-stack' label, BUT there's a bigger issue:\n\n**Router mode architecture:**\n- Router (port 8080) → does NOT expose /metrics endpoint\n- Child servers (dynamic ports) → expose metrics, but ports are random and no model label\n\n## Solution: Metrics Exporter Sidecar\nCreate a sidecar container that:\n1. Queries router's /v1/models to discover loaded models and their ports\n2. Scrapes each child server's /metrics endpoint\n3. Adds model name label to metrics\n4. Exposes consolidated metrics on a fixed port (e.g., 9090)\n\n## Implementation\n1. Create Python/Go exporter script\n2. Add as sidecar container to llama-server deployment\n3. Update ServiceMonitor to scrape sidecar port\n4. Add release: kube-prometheus-stack label\n\n## Metrics to expose (with model label)\n- llamacpp:prompt_tokens_total{model=\"...\"}\n- llamacpp:tokens_predicted_total{model=\"...\"}\n- llamacpp:prompt_tokens_seconds{model=\"...\"}\n- llamacpp:predicted_tokens_seconds{model=\"...\"}\n- llamacpp:requests_processing{model=\"...\"}\n- llamacpp:requests_deferred{model=\"...\"}\n- llamacpp:n_busy_slots_per_decode{model=\"...\"}","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-28T23:25:43.067638662Z","updated_at":"2025-12-29T00:05:17.495465118Z","closed_at":"2025-12-29T00:05:17.495465118Z","close_reason":"Implemented metrics exporter sidecar that scrapes child servers and adds model labels. Prometheus now scraping successfully - verified with query showing model='Llama-3.2-3B-Instruct-Q4_K_M' label."}
{"id":"eh-ops-4n6","title":"Deploy MetalLB L2 LoadBalancer","description":"Replace Cilium L2 announcements with MetalLB L2 mode. Cilium L2 announcements are broken on kernel 6.18.1 due to TC/BPF datapath issues. MetalLB uses userspace ARP daemon which bypasses the kernel BPF issue. Tasks: 1) Add MetalLB Helm chart to infrastructure 2) Configure IPAddressPool for 192.168.1.64/28 and 192.168.1.80/28 ranges 3) Configure L2Advertisement resources 4) Test LoadBalancer services work externally","status":"tombstone","priority":1,"issue_type":"task","created_at":"2025-12-26T13:43:26.539116336Z","updated_at":"2025-12-26T22:54:24.677788481Z","close_reason":"Root cause identified as router ARP issue, not kernel BPF. Reverting to Cilium L2 announcements.","deleted_at":"2025-12-26T22:54:24.677788481Z","deleted_by":"vscode","delete_reason":"manual delete","original_type":"task"}
{"id":"eh-ops-5ni","title":"Configure Talos machine configs for all nodes","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-23T08:44:10.390118604-07:00","updated_at":"2025-12-23T17:33:36.28451221Z","closed_at":"2025-12-23T17:33:36.28451221Z","close_reason":"Completed: Copied talconfig.yaml, talsecret.sops.yaml, and decrypted talsecrets.yaml from eh-ops-repo/talos/ along with all node configs in clusterconfig/. Upgraded Talos v1.11.1→v1.12.0 and Kubernetes v1.34.0→v1.35.0"}
{"id":"eh-ops-63k","title":"Deploy KEDA for autoscaling","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-23T08:46:23.033080009-07:00","updated_at":"2025-12-28T22:29:33.460333933Z","closed_at":"2025-12-28T22:29:33.460333933Z","close_reason":"Not needed - single GPU setup with direct llama.cpp server doesn't benefit from KEDA. Original plan assumed KServe architecture which was replaced with simpler direct deployment.","dependencies":[{"issue_id":"eh-ops-63k","depends_on_id":"eh-ops-e6a","type":"blocks","created_at":"2025-12-23T08:49:02.976616694-07:00","created_by":"micknugget","metadata":"{}"}]}
{"id":"eh-ops-7h6","title":"Create Grafana dashboard for llama.cpp metrics","description":"Create a Grafana dashboard to visualize llama.cpp inference metrics.\n\n## Metrics to Display\n- Token throughput: rate(llamacpp:tokens_predicted_total)\n- Prompt processing: rate(llamacpp:prompt_tokens_total)\n- Generation speed: llamacpp:predicted_tokens_seconds\n- Prompt speed: llamacpp:prompt_tokens_seconds\n- Slot utilization: llamacpp:n_busy_slots_per_decode\n\n## Panels\n1. Tokens/sec (generation + prompt) - time series\n2. Total tokens processed - stat panel\n3. Slot utilization gauge\n4. Request latency derived from counters\n\n## Implementation\n- Create ConfigMap with dashboard JSON\n- Add to Grafana via sidecar or provisioning\n- No pre-built dashboard exists - must create custom\n\n## References\n- llama.cpp metrics: /metrics endpoint on port 8080\n- Similar to vLLM dashboard patterns","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-28T23:25:49.218674454Z","updated_at":"2025-12-29T00:14:54.362176501Z","closed_at":"2025-12-29T00:14:54.362176501Z","close_reason":"Dashboard deployed to Grafana. Includes overview stats, throughput graphs, performance trends, and queue/slot panels. Model selector variable for filtering. Available in Infrastructure folder.","dependencies":[{"issue_id":"eh-ops-7h6","depends_on_id":"eh-ops-38u","type":"blocks","created_at":"2025-12-28T23:26:03.66960345Z","created_by":"vscode"}]}
{"id":"eh-ops-9ez","title":"Layer 3: AI/ML Applications","status":"closed","priority":3,"issue_type":"epic","created_at":"2025-12-24T17:06:04.045475779Z","updated_at":"2025-12-28T23:19:33.133547535Z","closed_at":"2025-12-28T23:19:33.133547535Z","close_reason":"Done - AI/ML layer deployed: llama.cpp server with Vulkan backend, OpenAI-compatible API via HTTPRoute. KEDA/KServe/Gateway Inference Extension deemed unnecessary for single-GPU setup.","dependencies":[{"issue_id":"eh-ops-9ez","depends_on_id":"eh-ops-oxx","type":"blocks","created_at":"2025-12-24T17:07:05.775904071Z","created_by":"vscode"}]}
{"id":"eh-ops-9zq","title":"Verify cluster health and connectivity","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-23T08:44:23.051942238-07:00","updated_at":"2025-12-24T17:57:41.262373018Z","closed_at":"2025-12-24T17:57:41.262373018Z","close_reason":"Verified cluster health: All 8 nodes joined and healthy. Control plane components (API server, controller manager, scheduler, etcd) running. Kube-proxy on all nodes. CoreDNS pending as expected (no CNI yet).","dependencies":[{"issue_id":"eh-ops-9zq","depends_on_id":"eh-ops-jlu","type":"blocks","created_at":"2025-12-23T08:47:57.1372975-07:00","created_by":"micknugget","metadata":"{}"}]}
{"id":"eh-ops-d9s","title":"Deploy Prometheus with custom scrape configs","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-23T08:45:34.013507391-07:00","updated_at":"2025-12-27T03:45:19.333791771Z","closed_at":"2025-12-27T03:45:19.333791771Z","close_reason":"Closed","dependencies":[{"issue_id":"eh-ops-d9s","depends_on_id":"eh-ops-mhv","type":"blocks","created_at":"2025-12-23T08:49:24.772894985-07:00","created_by":"micknugget","metadata":"{}"}]}
{"id":"eh-ops-did","title":"Deploy AlertManager with webhook routing","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-23T08:45:47.052434071-07:00","updated_at":"2025-12-27T17:54:18.301912286Z","closed_at":"2025-12-27T17:54:18.301912286Z","close_reason":"Closed","dependencies":[{"issue_id":"eh-ops-did","depends_on_id":"eh-ops-0xd","type":"blocks","created_at":"2025-12-23T08:48:46.451874087-07:00","created_by":"micknugget","metadata":"{}"}]}
{"id":"eh-ops-e6a","title":"Deploy KServe with llama.cpp runtime","description":"Deploy KServe with custom llama.cpp runtime for LLM inference on AMD Strix Halo.\n\n## Research Findings (Dec 2025)\n\n### Current Versions\n| Component | Version | Notes |\n|-----------|---------|-------|\n| KServe | v0.16.0 | CNCF Incubating (Nov 2025) |\n| Gateway API | v1.2.1 | Recommended over Istio |\n| Gateway API Inference Extension | v1.2.1 | GA (was alpha in ADR-003) |\n| KEDA | 2.18+ | For LLM-specific autoscaling |\n\n### Architecture (from ADR-003, updated)\n1. **KServe Standard Mode** - Not Knative, uses raw Kubernetes Deployments\n2. **Custom llama.cpp runtime** - GGUF format, ROCm 6.4+ for gfx1151\n3. **Gateway API** - Already deployed (Cilium Gateway)\n4. **Gateway API Inference Extension** - Now GA, provides InferencePool CRD\n5. **KEDA** - Blocked on this issue (eh-ops-63k)\n\n### Key Changes from Original ADR\n- Gateway API Inference Extension is now GA (v1.2.1)\n- KServe v0.16.0 adds LLMInferenceService controller\n- AMD has official KServe integration (AIM - AMD Inference Microservice)\n\n### llama.cpp Optimization for Strix Halo\nFrom strixhalo-homelab research:\n- Always use `--no-mmap` on ROCm (critical!)\n- Use `-ngl 999` to offload all layers to GPU\n- Enable Flash Attention with rocWMMA builds\n- 85W power mode is sweet spot (memory-bandwidth limited)\n- Docker images: kyuz0/amd-strix-halo-toolboxes\n\n## Blocked By\n- eh-ops-tgc: Enable AMD GPU on shadow node (kernel params, device plugin)\n\n## Implementation Plan (after GPU ready)\n1. Deploy KServe controller (HelmRelease)\n2. Create ClusterServingRuntime for llama.cpp\n3. Configure model storage PVC (NFS to shadow)\n4. Deploy InferenceService for test model\n5. Add ServiceMonitor for Prometheus metrics\n6. (Optional) Gateway API Inference Extension for intelligent routing\n\n## References\n- ADR: /workspace/eh-ops-repo/docs/adr/003-kserve-llm-serving.md\n- Example: /workspace/eh-ops-repo/docker/kserve-llamacpp/kserve-example.yaml\n- strixhalo-homelab: https://github.com/deseven/strixhalo-homelab\n\n---\n\n## Progress (Dec 28, 2025)\n\n### Completed\n- Deployed llama.cpp server using kyuz0's `rocm-7.1.1-rocwmma` image\n- Router mode enabled (`--models-dir /models --models-max 3`) for multi-model with LRU eviction\n- Critical ROCm flags configured: `--no-mmap -ngl 999 -fa 1 --cont-batching`\n- 500Gi local PVC for model storage (volume expansion enabled)\n- HTTPRoute via Cilium Gateway at `llm.${DOMAIN1}`\n- ServiceMonitor for Prometheus metrics scraping\n- Commit: f36fd79\n\n### Pending\n- Download test models to PVC\n- Verify inference works end-to-end\n- Test multi-model loading/swapping\n- Benchmark performance\n- (Future) Gateway API Inference Extension - requires non-Cilium gateway\n- (Future) KEDA autoscaling integration\n- (Future) Sophisticated model management system","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-23T08:46:06.20992832-07:00","updated_at":"2025-12-28T03:30:51.063210121Z","closed_at":"2025-12-28T03:30:51.063210121Z","close_reason":"llama.cpp deployed with Vulkan backend on Strix Halo (Radeon 8060S).\n\nKey findings:\n- ROCm hangs on gfx1151 during GPU upload (known issue)\n- Vulkan RADV backend works correctly\n- Router mode also crashed, using single-model mode\n\nPerformance (Llama-3.2-3B-Instruct-Q4_K_M):\n- Prompt: 721 t/s\n- Generation: 63 t/s\n\nFiles created:\n- kubernetes/infrastructure/llama-cpp/*\n\nNote: Decided not to use KServe - llama.cpp server provides OpenAI-compatible API directly","dependencies":[{"issue_id":"eh-ops-e6a","depends_on_id":"eh-ops-eyu","type":"blocks","created_at":"2025-12-23T08:48:57.894584087-07:00","created_by":"micknugget","metadata":"{}"},{"issue_id":"eh-ops-e6a","depends_on_id":"eh-ops-tgc","type":"blocks","created_at":"2025-12-27T23:57:53.459366864Z","created_by":"vscode"}]}
{"id":"eh-ops-eyu","title":"Deploy Spegel P2P image registry","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-23T08:45:59.803903913-07:00","updated_at":"2025-12-27T23:42:34.879389783Z","closed_at":"2025-12-27T23:42:34.879389783Z","close_reason":"Closed","dependencies":[{"issue_id":"eh-ops-eyu","depends_on_id":"eh-ops-2k9","type":"blocks","created_at":"2025-12-23T08:49:29.856741785-07:00","created_by":"micknugget","metadata":"{}"}]}
{"id":"eh-ops-ezu","title":"Fix ROCm backend for llama.cpp on Strix Halo gfx1151","description":"# ROCm Backend Investigation for Strix Halo (gfx1151)\n\n## Goal\nGet ROCm backend working alongside Vulkan for llama.cpp on AMD Strix Halo (Radeon 8060S, gfx1151).\n\n## Current Status\n- **Vulkan RADV**: Working (721 t/s prompt, 63 t/s generation)\n- **ROCm**: Hangs during model loading at \"async uploads\" phase\n\n## Hardware\n- **CPU**: AMD Ryzen AI MAX+ 395\n- **GPU**: Radeon 8060S (gfx1151, 128GB UMA)\n- **Node**: shadow (workload-type: gpu)\n\n## Symptoms Observed\n\n### ROCm Hang\n```\nload_tensors: offloaded 29/29 layers to GPU\nload_tensors: CPU model buffer size = 308.23 MiB\nload_tensors: ROCm0 model buffer size = 1918.35 MiB\nload_all_data: using async uploads for device ROCm0, buffer type ROCm0, backend ROCm0\n[hangs indefinitely here]\n```\n\n### Router Mode Crash\nChild process dies silently during async upload phase - no error message, just stops producing logs.\n\n## Known Issues (From Research)\n\n### Issue #5534 - ROCm 7.0.2 crashes on gfx1151\nhttps://github.com/ROCm/ROCm/issues/5534\n- \"invalid device function\" error during MUL_MAT\n- Affects all models regardless of Flash Attention settings\n- Status: \"assessed\" but still open (Oct 2025)\n\n### Issue #15018 - Slow loading past 64GB\nhttps://github.com/ggml-org/llama.cpp/issues/15018\n- Affects models \u003e 64GB VRAM\n- Workaround: --no-mmap\n- Our model is only 2GB so shouldn't apply\n\n### Strix Halo Wiki Notes\nhttps://strixhalo.wiki/AI/llamacpp-with-ROCm\n- \"As of ROCm 7.0.2+ the ROCWMMA flag/path SHOULD NOT BE USED\"\n- ROCWMMA slower than regular ROCm/HIP path at context depth increases\n- gfx1100 kernels 2-6x faster than gfx1151 kernels\n\n## Images Tested\n\n| Image | Result |\n|-------|--------|\n| `kyuz0/amd-strix-halo-toolboxes:rocm-7.1.1-rocwmma` | Hangs at async upload |\n| `kyuz0/amd-strix-halo-toolboxes:rocm-7.1.1` | Hangs at async upload |\n| `kyuz0/amd-strix-halo-toolboxes:vulkan-radv` | **Works!** |\n\n## Flags Tested with ROCm\n\n### Required Flags (per wiki)\n- `--no-mmap` - Prevents catastrophic slowdown\n- `-fa 1` - Flash attention\n\n### Flags That Didn't Help\n- `-fit off` - Didn't prevent hang\n- `--verbose` - No additional error info before hang\n- `ROCBLAS_USE_HIPBLASLT=1` - Recommended but didn't fix hang\n\n## Investigation Areas\n\n### 1. ROCm Version Compatibility\n- Current: ROCm 7.1.1 in kyuz0 image\n- Check if newer ROCm versions fix gfx1151 issues\n- Check lemonade-sdk/llamacpp-rocm for nightly builds\n\n### 2. Kernel Compilation\n- gfx1151 kernels may need JIT compilation\n- Check if there's a kernel cache that can be pre-warmed\n- Investigate ROCM_PATH and HIP_CLANG_PATH settings\n\n### 3. Memory Allocation\n- UMA (Unified Memory Architecture) handling\n- hipMemcpy on pageable memory causes page locking issues\n- Check GGML_VK_FORCE_MAX_ALLOCATION_SIZE equivalent for ROCm\n\n### 4. Alternative Approaches\n- Try gfx1100 target (reported 2-6x faster but may not work)\n- Use sirmo/llamacpp_rocm_minimal image\n- Build custom llama.cpp with specific flags\n\n## Resources\n\n### Docker Images\n- https://hub.docker.com/r/kyuz0/amd-strix-halo-toolboxes/tags\n- https://github.com/lemonade-sdk/llamacpp-rocm\n- https://github.com/sirmo/llamacpp_rocm_minimal\n\n### Documentation\n- https://strixhalo.wiki/AI/llamacpp-with-ROCm\n- https://rocm.docs.amd.com/projects/radeon-ryzen/en/latest/docs/advanced/advancedrad/windows/llm/llamacpp.html\n- https://github.com/ggml-org/llama.cpp/blob/master/tools/server/README.md\n\n### Build Commands (for reference)\n```bash\ncmake -B build -S . \\\n  -DGGML_HIP=ON \\\n  -DAMDGPU_TARGETS=\"gfx1151\" \\\n  -DCMAKE_BUILD_TYPE=Release\n\n# Optional (NOT recommended per wiki):\n# -DGGML_HIP_ROCWMMA_FATTN=ON\n```\n\n## Current Deployment\n- Namespace: llama-cpp\n- Image: docker.io/kyuz0/amd-strix-halo-toolboxes:vulkan-radv\n- Model: Llama-3.2-3B-Instruct-Q4_K_M.gguf (2GB)\n\n## Success Criteria\n1. ROCm backend loads model without hanging\n2. Inference works with comparable or better performance than Vulkan\n3. Can switch between Vulkan and ROCm via image tag change","notes":"ROCm hangs at graph_reserve on gfx1151. Tested: ROCm 7.1.1, 7alpha nightly, gfx1100 override, flash attention on/off - all hang. HSA_ENABLE_SDMA=0 fixes async upload hang but kernel execution still fails. See kubernetes/infrastructure/llama-cpp-rocm-test/ROCM_INVESTIGATION.md. Vulkan works (721 t/s prompt). Waiting for ROCm fix.","status":"blocked","priority":2,"issue_type":"task","created_at":"2025-12-28T03:33:32.504864438Z","updated_at":"2025-12-28T15:10:37.587447252Z","dependencies":[{"issue_id":"eh-ops-ezu","depends_on_id":"eh-ops-9ez","type":"blocks","created_at":"2025-12-28T03:34:34.1458525Z","created_by":"vscode"}]}
{"id":"eh-ops-hre","title":"Deploy LLM inference services and HTTPRoutes","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-23T08:46:35.69950996-07:00","updated_at":"2025-12-28T23:06:22.800878655Z","closed_at":"2025-12-28T23:06:22.800878655Z","close_reason":"Done - llama.cpp server deployed with HTTPRoute at llm.${DOMAIN1}. OpenAI-compatible API accessible via Cilium Gateway.","dependencies":[{"issue_id":"eh-ops-hre","depends_on_id":"eh-ops-1be","type":"blocks","created_at":"2025-12-23T08:49:13.148820069-07:00","created_by":"micknugget","metadata":"{}"}]}
{"id":"eh-ops-j12","title":"Deploy Gateway API and external gateway","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T08:44:58.580284339-07:00","updated_at":"2025-12-26T23:03:51.794488159Z","closed_at":"2025-12-26T23:03:51.794488159Z","close_reason":"Gateway API deployed with Cilium controller. GatewayClass 'cilium' and external-gateway on 192.168.1.65:80 working. HTTP 404 returned (expected - no routes). Note: Cilium agent restart may be needed after L2 lease failover to reprogram L7LB proxy.","dependencies":[{"issue_id":"eh-ops-j12","depends_on_id":"eh-ops-sue","type":"blocks","created_at":"2025-12-23T08:48:14.717153121-07:00","created_by":"micknugget","metadata":"{}"}]}
{"id":"eh-ops-jlu","title":"Bootstrap Talos cluster nodes","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-23T08:44:16.728792637-07:00","updated_at":"2025-12-24T17:50:17.377380622Z","closed_at":"2025-12-24T17:50:17.377380622Z","close_reason":"Completed: Successfully bootstrapped Talos v1.12.0 cluster with 7 nodes (3 control plane + 4 workers). All nodes running, CSRs approved, control plane healthy.","dependencies":[{"issue_id":"eh-ops-jlu","depends_on_id":"eh-ops-5ni","type":"blocks","created_at":"2025-12-23T08:47:52.044882324-07:00","created_by":"micknugget","metadata":"{}"}]}
{"id":"eh-ops-lhm","title":"Deploy k8s-gateway for Tailscale DNS","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-27T13:48:11.634566993Z","updated_at":"2025-12-27T13:51:22.33896893Z","closed_at":"2025-12-27T13:51:22.33896893Z","close_reason":"Closed"}
{"id":"eh-ops-mhv","title":"Configure Cloudflare Tunnel ingress","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T08:45:27.294130618-07:00","updated_at":"2025-12-27T02:11:27.782993902Z","closed_at":"2025-12-27T02:11:27.782993902Z","close_reason":"Closed","dependencies":[{"issue_id":"eh-ops-mhv","depends_on_id":"eh-ops-2ek","type":"blocks","created_at":"2025-12-23T08:48:35.051531821-07:00","created_by":"micknugget","metadata":"{}"}]}
{"id":"eh-ops-oxx","title":"Layer 1: Kubernetes Infrastructure","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-24T17:05:28.650350178Z","updated_at":"2025-12-28T23:18:48.022748791Z","closed_at":"2025-12-28T23:18:48.022748791Z","close_reason":"Done - Layer 1 infrastructure fully deployed: Cilium, cert-manager, metrics-server, k8s-gateway, Kyverno, NFS storage, Tailscale, Spegel, AMD GPU plugin. All HelmReleases healthy.","dependencies":[{"issue_id":"eh-ops-oxx","depends_on_id":"eh-ops-09p","type":"blocks","created_at":"2025-12-24T17:06:24.880089673Z","created_by":"vscode"}]}
{"id":"eh-ops-sue","title":"Deploy NGINX ingress controller","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T08:44:45.326503917-07:00","updated_at":"2025-12-26T21:02:21.742060763Z","closed_at":"2025-12-26T21:02:21.742060763Z","close_reason":"Closed","dependencies":[{"issue_id":"eh-ops-sue","depends_on_id":"eh-ops-wvt","type":"blocks","created_at":"2025-12-23T08:48:09.639910247-07:00","created_by":"micknugget","metadata":"{}"}]}
{"id":"eh-ops-tgc","title":"Enable AMD GPU on shadow node (Strix Halo)","description":"Enable AMD Strix Halo GPU (Radeon 8060S / gfx1151) on shadow node for LLM inference workloads.\n\n## Hardware\n- CPU: AMD Ryzen AI MAX+ 395\n- GPU: Radeon 8060S (gfx1151) integrated\n- Memory: 128GB Unified Memory Architecture (UMA)\n- ROCm target: gfx1151\n\n## Requirements\n\n### 1. Kernel Parameters (DONE in talconfig.yaml)\n- `ttm.pages_limit=31457280` (120GB for GPU, leaves 8GB for system)\n- `amd_iommu=off` (~6% faster GPU memory reads)\n\n### 2. Talos Factory Image with ROCm (TODO)\nCurrent image lacks ROCm drivers. Need to:\n- Create factory image with AMDGPU/ROCm extensions\n- Or use system extensions approach\n\n### 3. AMD GPU Device Plugin (TODO)\nDeploy to kubernetes/infrastructure/:\n- AMD GPU device plugin DaemonSet\n- Enables `amd.com/gpu` resource requests\n\n### 4. Verification Steps\n- [ ] Apply updated talconfig to shadow node\n- [ ] Verify `/dev/dri/renderD128` is available\n- [ ] Run `rocm-smi` or equivalent diagnostic\n- [ ] Test llama.cpp inference with GPU offload\n\n## References\n- https://github.com/deseven/strixhalo-homelab\n- ROCm 6.4.3+ has gfx1151 support\n- Docker images: kyuz0/amd-strix-halo-toolboxes","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-27T23:57:38.098051691Z","updated_at":"2025-12-28T00:43:40.04256648Z","closed_at":"2025-12-28T00:43:40.04256648Z","close_reason":"GPU fully operational - amd.com/gpu resource available, 120GB VRAM, device plugin and labeller running"}
{"id":"eh-ops-vu9","title":"Layer 2: Observability Stack","status":"closed","priority":2,"issue_type":"epic","created_at":"2025-12-24T17:05:53.28121397Z","updated_at":"2025-12-28T23:19:27.178407179Z","closed_at":"2025-12-28T23:19:27.178407179Z","close_reason":"Done - Observability stack deployed: kube-prometheus-stack and Grafana running in observability namespace.","dependencies":[{"issue_id":"eh-ops-vu9","depends_on_id":"eh-ops-oxx","type":"blocks","created_at":"2025-12-24T17:06:55.363568384Z","created_by":"vscode"}]}
{"id":"eh-ops-wvt","title":"Deploy cert-manager with Let's Encrypt","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T08:44:35.946582052-07:00","updated_at":"2025-12-26T20:51:59.0186129Z","closed_at":"2025-12-26T20:51:59.0186129Z","close_reason":"Closed","dependencies":[{"issue_id":"eh-ops-wvt","depends_on_id":"eh-ops-0aq","type":"blocks","created_at":"2025-12-23T08:48:04.556416739-07:00","created_by":"micknugget","metadata":"{}"}]}
